{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "127c21c4",
   "metadata": {},
   "source": [
    "-- Notepad to myself --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a287377",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ce577cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe1f6fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Case Number: string (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Block: string (nullable = true)\n",
      " |-- IUCR: string (nullable = true)\n",
      " |-- Primary Type: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Location Description: string (nullable = true)\n",
      " |-- Arrest: boolean (nullable = true)\n",
      " |-- Domestic: boolean (nullable = true)\n",
      " |-- Beat: integer (nullable = true)\n",
      " |-- District: integer (nullable = true)\n",
      " |-- Ward: integer (nullable = true)\n",
      " |-- Community Area: integer (nullable = true)\n",
      " |-- FBI Code: string (nullable = true)\n",
      " |-- X Coordinate: integer (nullable = true)\n",
      " |-- Y Coordinate: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Updated On: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col\n",
    "df = spark.read.csv('data/Crimes-2021.csv', header=True, inferSchema=True) \\\n",
    "    .withColumn('Date', to_timestamp(col('Date'), 'MM/dd/yyyy hh:mm:ss a'))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733f56bd",
   "metadata": {},
   "source": [
    "### All the functions available to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aeefd67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Column', 'DataFrame', 'DataType', 'PandasUDFType', 'PythonEvalType', 'SparkContext', 'StringType', 'UserDefinedFunction', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_create_column_from_literal', '_create_lambda', '_create_udf', '_get_get_jvm_function', '_get_lambda_parameters', '_invoke_binary_math_function', '_invoke_function', '_invoke_function_over_column', '_invoke_higher_order_function', '_options_to_str', '_test', '_to_java_column', '_to_seq', '_unresolved_named_lambda_variable', 'abs', 'acos', 'acosh', 'add_months', 'aggregate', 'approxCountDistinct', 'approx_count_distinct', 'array', 'array_contains', 'array_distinct', 'array_except', 'array_intersect', 'array_join', 'array_max', 'array_min', 'array_position', 'array_remove', 'array_repeat', 'array_sort', 'array_union', 'arrays_overlap', 'arrays_zip', 'asc', 'asc_nulls_first', 'asc_nulls_last', 'ascii', 'asin', 'asinh', 'assert_true', 'atan', 'atan2', 'atanh', 'avg', 'base64', 'bin', 'bitwiseNOT', 'bitwise_not', 'broadcast', 'bround', 'bucket', 'cbrt', 'ceil', 'coalesce', 'col', 'collect_list', 'collect_set', 'column', 'concat', 'concat_ws', 'conv', 'corr', 'cos', 'cosh', 'count', 'countDistinct', 'count_distinct', 'covar_pop', 'covar_samp', 'crc32', 'create_map', 'cume_dist', 'current_date', 'current_timestamp', 'date_add', 'date_format', 'date_sub', 'date_trunc', 'datediff', 'dayofmonth', 'dayofweek', 'dayofyear', 'days', 'decode', 'degrees', 'dense_rank', 'desc', 'desc_nulls_first', 'desc_nulls_last', 'element_at', 'encode', 'exists', 'exp', 'explode', 'explode_outer', 'expm1', 'expr', 'factorial', 'filter', 'first', 'flatten', 'floor', 'forall', 'format_number', 'format_string', 'from_csv', 'from_json', 'from_unixtime', 'from_utc_timestamp', 'functools', 'get_json_object', 'greatest', 'grouping', 'grouping_id', 'hash', 'hex', 'hour', 'hours', 'hypot', 'initcap', 'input_file_name', 'instr', 'isnan', 'isnull', 'json_tuple', 'kurtosis', 'lag', 'last', 'last_day', 'lead', 'least', 'length', 'levenshtein', 'lit', 'locate', 'log', 'log10', 'log1p', 'log2', 'lower', 'lpad', 'ltrim', 'map_concat', 'map_entries', 'map_filter', 'map_from_arrays', 'map_from_entries', 'map_keys', 'map_values', 'map_zip_with', 'max', 'md5', 'mean', 'min', 'minute', 'monotonically_increasing_id', 'month', 'months', 'months_between', 'nanvl', 'next_day', 'nth_value', 'ntile', 'overlay', 'pandas_udf', 'percent_rank', 'percentile_approx', 'posexplode', 'posexplode_outer', 'pow', 'product', 'quarter', 'radians', 'raise_error', 'rand', 'randn', 'rank', 'regexp_extract', 'regexp_replace', 'repeat', 'reverse', 'rint', 'round', 'row_number', 'rpad', 'rtrim', 'schema_of_csv', 'schema_of_json', 'second', 'sentences', 'sequence', 'session_window', 'sha1', 'sha2', 'shiftLeft', 'shiftRight', 'shiftRightUnsigned', 'shiftleft', 'shiftright', 'shiftrightunsigned', 'shuffle', 'signum', 'sin', 'since', 'sinh', 'size', 'skewness', 'slice', 'sort_array', 'soundex', 'spark_partition_id', 'split', 'sqrt', 'stddev', 'stddev_pop', 'stddev_samp', 'struct', 'substring', 'substring_index', 'sum', 'sumDistinct', 'sum_distinct', 'sys', 'tan', 'tanh', 'timestamp_seconds', 'toDegrees', 'toRadians', 'to_csv', 'to_date', 'to_json', 'to_str', 'to_timestamp', 'to_utc_timestamp', 'transform', 'transform_keys', 'transform_values', 'translate', 'trim', 'trunc', 'udf', 'unbase64', 'unhex', 'unix_timestamp', 'upper', 'var_pop', 'var_samp', 'variance', 'warnings', 'weekofyear', 'when', 'window', 'xxhash64', 'year', 'years', 'zip_with']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "print(dir(functions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0bf59",
   "metadata": {},
   "source": [
    "### String Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca33d6cb",
   "metadata": {},
   "source": [
    "#### Display the Primary Type column in lower and upper characters, the first letter capitalized and the first 4 characters of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b76f295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, upper, initcap, substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26fa477a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function substring in module pyspark.sql.functions:\n",
      "\n",
      "substring(str, pos, len)\n",
      "    Substring starts at `pos` and is of length `len` when str is String type or\n",
      "    returns the slice of byte array that starts at `pos` in byte and is of length `len`\n",
      "    when str is Binary type.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The position is not zero based, but 1 based index.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "    >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\n",
      "    [Row(s='ab')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(substring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aee8a0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+--------------------------+--------------------------+-----------------------------+\n",
      "|lower(Primary Type)       |upper(Primary Type)       |initcap(Primary Type)     |substring(Primary Type, 1, 4)|\n",
      "+--------------------------+--------------------------+--------------------------+-----------------------------+\n",
      "|theft                     |THEFT                     |Theft                     |THEF                         |\n",
      "|other offense             |OTHER OFFENSE             |Other Offense             |OTHE                         |\n",
      "|offense involving children|OFFENSE INVOLVING CHILDREN|Offense Involving Children|OFFE                         |\n",
      "|theft                     |THEFT                     |Theft                     |THEF                         |\n",
      "|battery                   |BATTERY                   |Battery                   |BATT                         |\n",
      "+--------------------------+--------------------------+--------------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lower(col('Primary Type')), \n",
    "          upper(col('Primary Type')), \n",
    "          initcap(col('Primary Type')), \n",
    "          substring(col('Primary Type'), 1, 4)).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43bcfe9",
   "metadata": {},
   "source": [
    "#### To do some sort of padding (e.g. left-pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fbc38bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "723ee538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function lpad in module pyspark.sql.functions:\n",
      "\n",
      "lpad(col, len, pad)\n",
      "    Left-pad the string column to width `len` with `pad`.\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "    >>> df.select(lpad(df.s, 6, '#').alias('s')).collect()\n",
      "    [Row(s='##abcd')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lpad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1072cba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|District|lpad(District, 3, 0)|\n",
      "+--------+--------------------+\n",
      "|      22|                 022|\n",
      "|      15|                 015|\n",
      "|       5|                 005|\n",
      "|      31|                 031|\n",
      "|      10|                 010|\n",
      "|       2|                 002|\n",
      "|       4|                 004|\n",
      "|       7|                 007|\n",
      "|       3|                 003|\n",
      "|       1|                 001|\n",
      "+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lpad = df.select('District',\n",
    "                    lpad(col('District'), 3, '0'))\n",
    "df_lpad.distinct().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8bbb5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- District: integer (nullable = true)\n",
      " |-- lpad(District, 3, 0): string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lpad.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f0553b",
   "metadata": {},
   "source": [
    "Change the data type again into integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d2963d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- District: integer (nullable = true)\n",
      " |-- lpad(District, 3, 0): string (nullable = true)\n",
      " |-- District_formatted: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lpad = df_lpad.withColumn(\"District_formatted\", col(\"lpad(District, 3, 0)\").cast(\"int\"))\n",
    "df_lpad.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad26901f",
   "metadata": {},
   "source": [
    "### Numeric Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a5ff28",
   "metadata": {},
   "source": [
    "#### Show the oldest date and the most recent date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b745a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d915dac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|min(Date)          |max(Date)          |\n",
      "+-------------------+-------------------+\n",
      "|2021-01-01 00:00:00|2021-12-31 23:59:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(min(col('Date')), \n",
    "          max(col('Date'))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4500cb97",
   "metadata": {},
   "source": [
    "### Date Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d63677",
   "metadata": {},
   "source": [
    "#### What is 3 days earlier that the oldest date and 3 days later than the most recent date?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f41cafe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af821d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function date_add in module pyspark.sql.functions:\n",
      "\n",
      "date_add(start, days)\n",
      "    Returns the date that is `days` days after `start`\n",
      "    \n",
      "    .. versionadded:: 1.5.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "    >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n",
      "    [Row(next_date=datetime.date(2015, 4, 9))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(date_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c2dd1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function to_date in module pyspark.sql.functions:\n",
      "\n",
      "to_date(col, format=None)\n",
      "    Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.DateType`\n",
      "    using the optionally specified format. Specify formats according to `datetime pattern`_.\n",
      "    By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n",
      "    is omitted. Equivalent to ``col.cast(\"date\")``.\n",
      "    \n",
      "    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "    \n",
      "    .. versionadded:: 2.2.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "    >>> df.select(to_date(df.t).alias('date')).collect()\n",
      "    [Row(date=datetime.date(1997, 2, 28))]\n",
      "    \n",
      "    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "    >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()\n",
      "    [Row(date=datetime.date(1997, 2, 28))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fad82f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------+------------------+----------------------+\n",
      "|to_date(min(Date))|date_sub(min(Date), 3)|to_date(max(Date))|date_add(max(Date), 3)|\n",
      "+------------------+----------------------+------------------+----------------------+\n",
      "|2021-01-01        |2020-12-29            |2021-12-31        |2022-01-03            |\n",
      "+------------------+----------------------+------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_date(min(col('Date'))),\n",
    "          date_sub(min(col('Date')), 3), \n",
    "          to_date(max(col('Date'))),\n",
    "          date_add(max(col('Date')), 3)).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

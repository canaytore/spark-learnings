{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Notepad to myself --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL for DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying DataFrames with Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark_SQL\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_json_path = data_path + \"utilization.json\"\n",
    "df2 = spark.read.json(df2_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2_csv_path = data_path + \"utilization.csv\"\n",
    "#df2 = spark.read.csv(df2_csv_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cpu_utilization: double (nullable = true)\n",
      " |-- event_datetime: string (nullable = true)\n",
      " |-- free_memory: double (nullable = true)\n",
      " |-- server_id: long (nullable = true)\n",
      " |-- session_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|event_datetime     |free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|0.57           |03/05/2019 08:06:14|0.51       |100      |47           |\n",
      "|0.47           |03/05/2019 08:11:14|0.62       |100      |43           |\n",
      "|0.56           |03/05/2019 08:16:14|0.57       |100      |62           |\n",
      "|0.57           |03/05/2019 08:21:14|0.56       |100      |50           |\n",
      "|0.35           |03/05/2019 08:26:14|0.46       |100      |43           |\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+------------------+------------------+------------------+\n",
      "|summary|    cpu_utilization|     event_datetime|       free_memory|         server_id|     session_count|\n",
      "+-------+-------------------+-------------------+------------------+------------------+------------------+\n",
      "|  count|             500000|             500000|            500000|            500000|            500000|\n",
      "|   mean| 0.6205177400000123|               null|0.3791280999999977|             124.5|          69.59616|\n",
      "| stddev|0.15875173872912837|               null|0.1583093127837622|14.430884120553253|14.850676696352865|\n",
      "|    min|               0.22|03/05/2019 08:06:14|               0.0|               100|                32|\n",
      "|    max|                1.0|04/09/2019 01:22:46|              0.78|               149|               105|\n",
      "+-------+-------------------+-------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView(\"utilization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|\n",
      "|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|\n",
      "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|\n",
      "|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|\n",
      "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT * \\\n",
    "                   FROM utilization \\\n",
    "                   LIMIT 10\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|sid|sct|\n",
      "+---+---+\n",
      "|100| 47|\n",
      "|100| 43|\n",
      "|100| 62|\n",
      "|100| 50|\n",
      "|100| 43|\n",
      "|100| 48|\n",
      "|100| 58|\n",
      "|100| 58|\n",
      "|100| 62|\n",
      "|100| 45|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id as sid, session_count as sct \\\n",
    "                   FROM utilization \\\n",
    "                   LIMIT 10\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering DataFrames with Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|event_datetime     |free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|0.66           |03/05/2019 08:06:48|0.31       |120      |54           |\n",
      "|0.58           |03/05/2019 08:11:48|0.38       |120      |64           |\n",
      "|0.55           |03/05/2019 08:16:48|0.61       |120      |54           |\n",
      "|0.7            |03/05/2019 08:21:48|0.35       |120      |80           |\n",
      "|0.6            |03/05/2019 08:26:48|0.39       |120      |71           |\n",
      "|0.53           |03/05/2019 08:31:48|0.35       |120      |49           |\n",
      "|0.73           |03/05/2019 08:36:48|0.42       |120      |73           |\n",
      "|0.41           |03/05/2019 08:41:48|0.6        |120      |72           |\n",
      "|0.62           |03/05/2019 08:46:48|0.57       |120      |57           |\n",
      "|0.67           |03/05/2019 08:51:48|0.44       |120      |78           |\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT * \\\n",
    "                   FROM utilization \\\n",
    "                   WHERE server_id = 120\")\n",
    "df_sql.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|server_id|session_count|\n",
      "+---------+-------------+\n",
      "|      100|           71|\n",
      "|      100|           71|\n",
      "|      100|           71|\n",
      "|      100|           71|\n",
      "|      100|           72|\n",
      "+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id, session_count \\\n",
    "                   FROM utilization \\\n",
    "                   WHERE session_count > 70\")\n",
    "df_sql.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239659"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|server_id|session_count|\n",
      "+---------+-------------+\n",
      "|      120|           80|\n",
      "|      120|           71|\n",
      "|      120|           73|\n",
      "|      120|           72|\n",
      "|      120|           78|\n",
      "+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id, session_count \\\n",
    "                   FROM utilization \\\n",
    "                   WHERE session_count > 70 AND server_id = 120\")\n",
    "df_sql.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2733"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|server_id|session_count|\n",
      "+---------+-------------+\n",
      "|      120|           80|\n",
      "|      120|           80|\n",
      "|      120|           80|\n",
      "|      120|           80|\n",
      "|      120|           80|\n",
      "+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id, session_count \\\n",
    "                    FROM utilization \\\n",
    "                    WHERE session_count > 70 AND server_id = 120 \\\n",
    "                    ORDER BY session_count DESC\")\n",
    "df_sql.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating DataFrames with Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  500000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT count(*) \\\n",
    "                   FROM utilization\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  239659|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT count(*) \\\n",
    "                    FROM utilization \\\n",
    "                    WHERE session_count > 70\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|server_id|count(1)|\n",
      "+---------+--------+\n",
      "|      107|    5646|\n",
      "|      103|    8744|\n",
      "|      104|    7366|\n",
      "|      100|     391|\n",
      "|      105|    1110|\n",
      "|      108|    8375|\n",
      "|      101|    9808|\n",
      "|      102|    8586|\n",
      "|      112|    7425|\n",
      "|      113|    9418|\n",
      "+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id, count(*) \\\n",
    "                    FROM utilization \\\n",
    "                    WHERE session_count > 70 \\\n",
    "                    GROUP BY server_id\")\n",
    "df_sql.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|server_id|count(1)|\n",
      "+---------+--------+\n",
      "|      101|    9808|\n",
      "|      113|    9418|\n",
      "|      145|    9304|\n",
      "|      103|    8744|\n",
      "|      102|    8586|\n",
      "|      133|    8583|\n",
      "|      108|    8375|\n",
      "|      149|    8288|\n",
      "|      137|    8248|\n",
      "|      148|    8027|\n",
      "+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id, count(*) \\\n",
    "                    FROM utilization \\\n",
    "                    WHERE session_count > 70 \\\n",
    "                    GROUP BY server_id \\\n",
    "                    ORDER BY count(*) DESC\")\n",
    "df_sql.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+------------------+------------------+\n",
      "|server_id|min(session_count)|avg(session_count)|max(session_count)|\n",
      "+---------+------------------+------------------+------------------+\n",
      "|      101|                71| 87.66557911908646|               105|\n",
      "|      113|                71| 86.96262476109577|               103|\n",
      "|      145|                71| 86.97732158211522|               103|\n",
      "|      103|                71| 85.76372369624886|               101|\n",
      "|      102|                71| 85.71150710458886|               101|\n",
      "|      133|                71| 85.46720260981009|               100|\n",
      "|      108|                71|  85.1219104477612|               100|\n",
      "|      149|                71|  84.9612693050193|                99|\n",
      "|      137|                71|  85.0061833171678|                99|\n",
      "|      148|                71| 84.70350068518749|                99|\n",
      "+---------+------------------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id, min(session_count), avg(session_count), max(session_count) \\\n",
    "                    FROM utilization \\\n",
    "                    WHERE session_count > 70 \\\n",
    "                    GROUP BY server_id \\\n",
    "                    ORDER BY count(*) DESC\")\n",
    "df_sql.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+\n",
      "|server_id|min_sc|avg_sc|max_sc|\n",
      "+---------+------+------+------+\n",
      "|      101|    71| 87.67|   105|\n",
      "|      113|    71| 86.96|   103|\n",
      "|      145|    71| 86.98|   103|\n",
      "|      103|    71| 85.76|   101|\n",
      "|      102|    71| 85.71|   101|\n",
      "|      133|    71| 85.47|   100|\n",
      "|      108|    71| 85.12|   100|\n",
      "|      149|    71| 84.96|    99|\n",
      "|      137|    71| 85.01|    99|\n",
      "|      148|    71|  84.7|    99|\n",
      "+---------+------+------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT server_id, min(session_count) min_sc, round(avg(session_count),2) avg_sc, max(session_count) max_sc \\\n",
    "                    FROM utilization \\\n",
    "                    WHERE session_count > 70 \\\n",
    "                    GROUP BY server_id \\\n",
    "                    ORDER BY count(*) DESC\")\n",
    "df_sql.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining DataFrames with Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|event_datetime     |free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|0.57           |03/05/2019 08:06:14|0.51       |100      |47           |\n",
      "|0.47           |03/05/2019 08:11:14|0.62       |100      |43           |\n",
      "|0.56           |03/05/2019 08:16:14|0.57       |100      |62           |\n",
      "|0.57           |03/05/2019 08:21:14|0.56       |100      |50           |\n",
      "|0.35           |03/05/2019 08:26:14|0.46       |100      |43           |\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5, truncate=False) #utilization table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_path = data_path + \"server_name.csv\"\n",
    "df3 = spark.read.csv(df3_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|server_id|server_name|\n",
      "+---------+-----------+\n",
      "|      100| 100 Server|\n",
      "|      101| 101 Server|\n",
      "|      102| 102 Server|\n",
      "|      103| 103 Server|\n",
      "|      104| 104 Server|\n",
      "+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.createOrReplaceTempView(\"server_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT server_id)|\n",
      "+-------------------------+\n",
      "|                       50|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT COUNT(DISTINCT server_id) \\\n",
    "                   FROM utilization\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|min(server_id)|max(server_id)|\n",
      "+--------------+--------------+\n",
      "|           100|           149|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT min(server_id), max(server_id) \\\n",
    "                   FROM utilization\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT server_id)|\n",
      "+-------------------------+\n",
      "|                       50|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT COUNT(DISTINCT server_id) \\\n",
    "                   FROM server_name\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|min(server_id)|max(server_id)|\n",
      "+--------------+--------------+\n",
      "|           100|           149|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT min(server_id), max(server_id) \\\n",
    "                   FROM server_name\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------+\n",
      "|server_id|server_name|session_count|\n",
      "+---------+-----------+-------------+\n",
      "|      100| 100 Server|           47|\n",
      "|      100| 100 Server|           43|\n",
      "|      100| 100 Server|           62|\n",
      "|      100| 100 Server|           50|\n",
      "|      100| 100 Server|           43|\n",
      "|      100| 100 Server|           48|\n",
      "|      100| 100 Server|           58|\n",
      "|      100| 100 Server|           58|\n",
      "|      100| 100 Server|           62|\n",
      "|      100| 100 Server|           45|\n",
      "+---------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join = spark.sql(\"SELECT u.server_id, sn.server_name, u.session_count \\\n",
    "                     FROM utilization u \\\n",
    "                     INNER JOIN server_name sn \\\n",
    "                     ON sn.server_id = u.server_id\")\n",
    "df_join.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+---------+-----------+\n",
      "|cpu_utilization|event_datetime     |free_memory|server_id|session_count|server_id|server_name|\n",
      "+---------------+-------------------+-----------+---------+-------------+---------+-----------+\n",
      "|0.57           |03/05/2019 08:06:14|0.51       |100      |47           |100      |100 Server |\n",
      "|0.47           |03/05/2019 08:11:14|0.62       |100      |43           |100      |100 Server |\n",
      "|0.56           |03/05/2019 08:16:14|0.57       |100      |62           |100      |100 Server |\n",
      "|0.57           |03/05/2019 08:21:14|0.56       |100      |50           |100      |100 Server |\n",
      "|0.35           |03/05/2019 08:26:14|0.46       |100      |43           |100      |100 Server |\n",
      "|0.41           |03/05/2019 08:31:14|0.58       |100      |48           |100      |100 Server |\n",
      "|0.57           |03/05/2019 08:36:14|0.35       |100      |58           |100      |100 Server |\n",
      "|0.41           |03/05/2019 08:41:14|0.4        |100      |58           |100      |100 Server |\n",
      "|0.53           |03/05/2019 08:46:14|0.35       |100      |62           |100      |100 Server |\n",
      "|0.51           |03/05/2019 08:51:14|0.6        |100      |45           |100      |100 Server |\n",
      "+---------------+-------------------+-----------+---------+-------------+---------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join = spark.sql(\"SELECT * \\\n",
    "                     FROM utilization u \\\n",
    "                     LEFT JOIN server_name sn \\\n",
    "                     ON sn.server_id = u.server_id\")\n",
    "df_join.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n",
      "|server_name|cpu_utilization|session_count|\n",
      "+-----------+---------------+-------------+\n",
      "| 101 Server|             85|           80|\n",
      "| 101 Server|             80|           90|\n",
      "| 102 Server|             85|           80|\n",
      "| 102 Server|             85|           80|\n",
      "+-----------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "df_dup = sc.parallelize([Row(server_name='101 Server', cpu_utilization=85, session_count=80), \\\n",
    "                         Row(server_name='101 Server', cpu_utilization=80, session_count=90),\n",
    "                         Row(server_name='102 Server', cpu_utilization=85, session_count=80),\n",
    "                         Row(server_name='102 Server', cpu_utilization=85, session_count=80)]).toDF()\n",
    "df_dup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n",
      "|server_name|cpu_utilization|session_count|\n",
      "+-----------+---------------+-------------+\n",
      "| 101 Server|             85|           80|\n",
      "| 101 Server|             80|           90|\n",
      "| 102 Server|             85|           80|\n",
      "+-----------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dup.drop_duplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n",
      "|server_name|cpu_utilization|session_count|\n",
      "+-----------+---------------+-------------+\n",
      "| 101 Server|             85|           80|\n",
      "| 102 Server|             85|           80|\n",
      "+-----------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dup.drop_duplicates(['server_name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sc.parallelize([Row(server_name='101 Server', cpu_utilization=85, session_count=80), \\\n",
    "                     Row(server_name='101 Server', cpu_utilization=80, session_count=90),\n",
    "                     Row(server_name='102 Server', cpu_utilization=85, session_count=40),\n",
    "                     Row(server_name='103 Server', cpu_utilization=70, session_count=80),\n",
    "                     Row(server_name='104 Server', cpu_utilization=60, session_count=80)]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n",
      "|server_name|cpu_utilization|session_count|\n",
      "+-----------+---------------+-------------+\n",
      "| 101 Server|             85|           80|\n",
      "| 101 Server|             80|           90|\n",
      "| 102 Server|             85|           40|\n",
      "| 103 Server|             70|           80|\n",
      "| 104 Server|             60|           80|\n",
      "+-----------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| 101 Server|             85|           80|  null|\n",
      "| 101 Server|             80|           90|  null|\n",
      "| 102 Server|             85|           40|  null|\n",
      "| 103 Server|             70|           80|  null|\n",
      "| 104 Server|             60|           80|  null|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_na = df.withColumn('na_col', lit(None).cast(StringType()))\n",
    "df_na.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| 101 Server|             85|           80|     Z|\n",
      "| 101 Server|             80|           90|     Z|\n",
      "| 102 Server|             85|           40|     Z|\n",
      "| 103 Server|             70|           80|     Z|\n",
      "| 104 Server|             60|           80|     Z|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_na.fillna('Z').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| 101 Server|             85|           80|     Z|\n",
      "| 101 Server|             80|           90|     Z|\n",
      "| 102 Server|             85|           40|     Z|\n",
      "| 103 Server|             70|           80|     Z|\n",
      "| 104 Server|             60|           80|     Z|\n",
      "| 101 Server|             85|           80|  null|\n",
      "| 101 Server|             80|           90|  null|\n",
      "| 102 Server|             85|           40|  null|\n",
      "| 103 Server|             70|           80|  null|\n",
      "| 104 Server|             60|           80|  null|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_na2 = df_na.fillna('Z').union(df_na)\n",
    "df_na2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| 101 Server|             85|           80|     Z|\n",
      "| 101 Server|             80|           90|     Z|\n",
      "| 102 Server|             85|           40|     Z|\n",
      "| 103 Server|             70|           80|     Z|\n",
      "| 104 Server|             60|           80|     Z|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_na2.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na2.createOrReplaceTempView(\"table_wNAs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| 101 Server|             85|           80|     Z|\n",
      "| 101 Server|             80|           90|     Z|\n",
      "| 102 Server|             85|           40|     Z|\n",
      "| 103 Server|             70|           80|     Z|\n",
      "| 104 Server|             60|           80|     Z|\n",
      "| 101 Server|             85|           80|  null|\n",
      "| 101 Server|             80|           90|  null|\n",
      "| 102 Server|             85|           40|  null|\n",
      "| 103 Server|             70|           80|  null|\n",
      "| 104 Server|             60|           80|  null|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * \\\n",
    "          FROM table_wNAs\") \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| 101 Server|             85|           80|  null|\n",
      "| 101 Server|             80|           90|  null|\n",
      "| 102 Server|             85|           40|  null|\n",
      "| 103 Server|             70|           80|  null|\n",
      "| 104 Server|             60|           80|  null|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * \\\n",
    "          FROM table_wNAs \\\n",
    "          WHERE na_col IS NULL\") \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| 101 Server|             85|           80|     Z|\n",
      "| 101 Server|             80|           90|     Z|\n",
      "| 102 Server|             85|           40|     Z|\n",
      "| 103 Server|             70|           80|     Z|\n",
      "| 104 Server|             60|           80|     Z|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * \\\n",
    "          FROM table_wNAs \\\n",
    "          WHERE na_col IS NOT NULL\") \\\n",
    ".show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cefc252b",
   "metadata": {},
   "source": [
    "-- Notepad to myself --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f74d7f",
   "metadata": {},
   "source": [
    "# Working with Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1388f",
   "metadata": {},
   "source": [
    "Both the *pandas* syntax and the *PySpark* syntax is provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0dcfa",
   "metadata": {},
   "source": [
    "### Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06ff2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5777e415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Case Number: string (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Block: string (nullable = true)\n",
      " |-- IUCR: string (nullable = true)\n",
      " |-- Primary Type: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Location Description: string (nullable = true)\n",
      " |-- Arrest: boolean (nullable = true)\n",
      " |-- Domestic: boolean (nullable = true)\n",
      " |-- Beat: integer (nullable = true)\n",
      " |-- District: integer (nullable = true)\n",
      " |-- Ward: integer (nullable = true)\n",
      " |-- Community Area: integer (nullable = true)\n",
      " |-- FBI Code: string (nullable = true)\n",
      " |-- X Coordinate: integer (nullable = true)\n",
      " |-- Y Coordinate: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Updated On: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col\n",
    "df = spark.read.csv('Crimes-2021.csv', header=True, inferSchema=True) \\\n",
    "    .withColumn('Date', to_timestamp(col('Date'),'MM/dd/yyyy hh:mm:ss a'))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e314c24e",
   "metadata": {},
   "source": [
    "### 1. Access a Column in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "123a7e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'Case Number',\n",
       " 'Date',\n",
       " 'Block',\n",
       " 'IUCR',\n",
       " 'Primary Type',\n",
       " 'Description',\n",
       " 'Location Description',\n",
       " 'Arrest',\n",
       " 'Domestic',\n",
       " 'Beat',\n",
       " 'District',\n",
       " 'Ward',\n",
       " 'Community Area',\n",
       " 'FBI Code',\n",
       " 'X Coordinate',\n",
       " 'Y Coordinate',\n",
       " 'Year',\n",
       " 'Updated On',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'Location']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd3a393",
   "metadata": {},
   "source": [
    "In PySpark, it's possible to access a DataFrame's column either by attribute (so a dot notation, or by indexing - where we would use square brackets). We cannot always use the dot notation since this will break when the column names have reserved names or attributes to the DataFrame Class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "709464a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Year'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19b8435c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Community Area'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Community Area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29525990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Year|\n",
      "+----+\n",
      "|2021|\n",
      "|2021|\n",
      "|2021|\n",
      "|2021|\n",
      "|2021|\n",
      "+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('Year')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae8aa7",
   "metadata": {},
   "source": [
    "### 1. Access a Column in pandas\n",
    "\n",
    "Similar rules apply in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd90c770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df2 = df.toPandas()\n",
    "type(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ec45065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Case Number', 'Date', 'Block', 'IUCR', 'Primary Type',\n",
       "       'Description', 'Location Description', 'Arrest', 'Domestic', 'Beat',\n",
       "       'District', 'Ward', 'Community Area', 'FBI Code', 'X Coordinate',\n",
       "       'Y Coordinate', 'Year', 'Updated On', 'Latitude', 'Longitude',\n",
       "       'Location'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b92fd022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2021\n",
       "1    2021\n",
       "2    2021\n",
       "3    2021\n",
       "4    2021\n",
       "Name: Year, dtype: int32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.Year.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "476e94d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207874    2021\n",
       "207875    2021\n",
       "207876    2021\n",
       "207877    2021\n",
       "207878    2021\n",
       "Name: Year, dtype: int32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['Year'].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1198df",
   "metadata": {},
   "source": [
    "### 2. Select multiple columns in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae9d184",
   "metadata": {},
   "source": [
    "In both PySpark and pandas, we can select more than one column using a list within square brackets. In PySpark, it's more common to use DataFrame .select() and then list the column names that we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af8c6a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+\n",
      "|Case Number|Year|\n",
      "+-----------+----+\n",
      "|   JE202728|2021|\n",
      "|   JF125633|2021|\n",
      "|   JE475344|2021|\n",
      "+-----------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Case Number', 'Year').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04d1db9",
   "metadata": {},
   "source": [
    "### 2. Select multiple columns in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cf78c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JE202728</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JF125633</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JE475344</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Case Number  Year\n",
       "0    JE202728  2021\n",
       "1    JF125633  2021\n",
       "2    JE475344  2021"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[['Case Number', 'Year']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f6646",
   "metadata": {},
   "source": [
    "### 3. Add a Column in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ad24a",
   "metadata": {},
   "source": [
    "To add a new column to our DataFrame, where the values in this new column are twice that of an existing column. In PySpark, we can use the withColumn() function. In pandas, we would specify the new name of the column, in square brackets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35d4f497",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('DoubleID', 2*df['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "279616eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=12846092, DoubleID=25692184),\n",
       " Row(ID=12841050, DoubleID=25682100),\n",
       " Row(ID=12839669, DoubleID=25679338)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('ID', 'DoubleID').tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838148e4",
   "metadata": {},
   "source": [
    "### 3. Add a Column in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58926286",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['DoubleID'] = df2['ID'].multiply(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0d5344e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DoubleID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>207876</th>\n",
       "      <td>12846092</td>\n",
       "      <td>25692184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207877</th>\n",
       "      <td>12841050</td>\n",
       "      <td>25682100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207878</th>\n",
       "      <td>12839669</td>\n",
       "      <td>25679338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID  DoubleID\n",
       "207876  12846092  25692184\n",
       "207877  12841050  25682100\n",
       "207878  12839669  25679338"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[['ID', 'DoubleID']].tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d70b2e0",
   "metadata": {},
   "source": [
    "### 4. Rename a Column in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3bd806",
   "metadata": {},
   "source": [
    "In PySpark, we can use the withColumnRenamed() function, providing the current column name as the first argument, and the new column name as the second. Renaming a column returns a new Dataframe. If we provide a column name that doesn't exist, then no operation is performed. In pandas, we can use the rename() function, specifying the column names to be changed as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44e00562",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('DoubleID', 'RenamedColumn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a52b728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'Case Number',\n",
       " 'Date',\n",
       " 'Block',\n",
       " 'IUCR',\n",
       " 'Primary Type',\n",
       " 'Description',\n",
       " 'Location Description',\n",
       " 'Arrest',\n",
       " 'Domestic',\n",
       " 'Beat',\n",
       " 'District',\n",
       " 'Ward',\n",
       " 'Community Area',\n",
       " 'FBI Code',\n",
       " 'X Coordinate',\n",
       " 'Y Coordinate',\n",
       " 'Year',\n",
       " 'Updated On',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'Location',\n",
       " 'RenamedColumn']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae70a3e",
   "metadata": {},
   "source": [
    "### 4. Rename a Column in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1981621",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.rename(columns={'DoubleID':'RenamedColumn'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "568bd4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Case Number', 'Date', 'Block', 'IUCR', 'Primary Type',\n",
       "       'Description', 'Location Description', 'Arrest', 'Domestic', 'Beat',\n",
       "       'District', 'Ward', 'Community Area', 'FBI Code', 'X Coordinate',\n",
       "       'Y Coordinate', 'Year', 'Updated On', 'Latitude', 'Longitude',\n",
       "       'Location', 'RenamedColumn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522ffaf",
   "metadata": {},
   "source": [
    "### 5. Remove Columns in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfc2d4a",
   "metadata": {},
   "source": [
    "Removing or dropping a column have similar syntax in both pandas and PySpark. With PySpark, we can use the drop() function and this returns a new DataFrame that drops the specified column. If the column that needs to be dropped doesn't exist, then no operation will be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "904f665d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'Case Number',\n",
       " 'Date',\n",
       " 'Block',\n",
       " 'IUCR',\n",
       " 'Primary Type',\n",
       " 'Description',\n",
       " 'Location Description',\n",
       " 'Arrest',\n",
       " 'Domestic',\n",
       " 'Beat',\n",
       " 'District',\n",
       " 'Ward',\n",
       " 'Community Area',\n",
       " 'FBI Code',\n",
       " 'X Coordinate',\n",
       " 'Y Coordinate',\n",
       " 'Year',\n",
       " 'Updated On',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'Location']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop('RenamedColumn')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d559a3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Block: string, IUCR: string, Primary Type: string, Description: string, Location Description: string, Arrest: boolean, Domestic: boolean, Beat: int, District: int, Ward: int, Community Area: int, FBI Code: string, X Coordinate: int, Y Coordinate: int, Year: int, Updated On: string, Latitude: double, Longitude: double, Location: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('ID', 'Case Number', 'Date') # multiple drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb646238",
   "metadata": {},
   "source": [
    "### 5. Remove Columns in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f1c8467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Case Number', 'Date', 'Block', 'IUCR', 'Primary Type',\n",
       "       'Description', 'Location Description', 'Arrest', 'Domestic', 'Beat',\n",
       "       'District', 'Ward', 'Community Area', 'FBI Code', 'X Coordinate',\n",
       "       'Y Coordinate', 'Year', 'Updated On', 'Latitude', 'Longitude',\n",
       "       'Location'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.drop(columns='RenamedColumn', inplace=True)\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2bb15a",
   "metadata": {},
   "source": [
    "### 6. Add a column with name 'One', with entries all 1s in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b0f4b6",
   "metadata": {},
   "source": [
    "Sometimes we might want to add a constant value for a column and we can do this using literals. So adding a column with name 'One' with entries all ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "886f83a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|One|\n",
      "+---+\n",
      "|  1|\n",
      "|  1|\n",
      "|  1|\n",
      "|  1|\n",
      "|  1|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df.withColumn('One', lit(1)).select('One').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc53219",
   "metadata": {},
   "source": [
    "### 6. Add a column with name 'One', with entries all 1s in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f5c6d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: One, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['One'] = 1\n",
    "df2['One'].head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

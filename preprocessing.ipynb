{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89d51618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('employee').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ddb561",
   "metadata": {},
   "source": [
    "Dataframes are a table-like data structure, and they have named columns. Dataframes are widely used in R and in the Python-pandas library. They're also used in Spark and they're similar to what's available in most Python and in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64ef57c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df = spark.read.csv(\"/employee.txt\", \n",
    "                        header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90a01820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- region_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d24f12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'last_name',\n",
       " 'email',\n",
       " 'gender',\n",
       " 'department',\n",
       " 'start_date',\n",
       " 'salary',\n",
       " 'job_title',\n",
       " 'region_id']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6517fff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='1', last_name=\"'Kelley'\", email=\"'rkelley0@soundcloud.com'\", gender=\"'Female'\", department=\"'Computers'\", start_date=\"'10/2/2009'\", salary='67470', job_title=\"'Structural Engineer'\", region_id='2'),\n",
       " Row(id='2', last_name=\"'Armstrong'\", email=\"'sarmstrong1@infoseek.co.jp'\", gender=\"'Male'\", department=\"'Sports'\", start_date=\"'3/31/2008'\", salary='71869', job_title=\"'Financial Advisor'\", region_id='2'),\n",
       " Row(id='3', last_name=\"'Carr'\", email=\"'fcarr2@woothemes.com'\", gender=\"'Male'\", department=\"'Automotive'\", start_date=\"'7/12/2009'\", salary='101768', job_title=\"'Recruiting Manager'\", region_id='3'),\n",
       " Row(id='4', last_name=\"'Murray'\", email=\"'jmurray3@gov.uk'\", gender=\"'Female'\", department=\"'Jewelery'\", start_date=\"'12/25/2014'\", salary='96897', job_title=\"'Desktop Support Technician'\", region_id='3'),\n",
       " Row(id='5', last_name=\"'Ellis'\", email=\"'jellis4@sciencedirect.com'\", gender=\"'Female'\", department=\"'Grocery'\", start_date=\"'9/19/2002'\", salary='63702', job_title=\"'Software Engineer III'\", region_id='7')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaf479fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+--------+------------+------------+------+--------------------+---------+\n",
      "| id|  last_name|               email|  gender|  department|  start_date|salary|           job_title|region_id|\n",
      "+---+-----------+--------------------+--------+------------+------------+------+--------------------+---------+\n",
      "|  1|   'Kelley'|'rkelley0@soundcl...|'Female'| 'Computers'| '10/2/2009'| 67470|'Structural Engin...|        2|\n",
      "|  2|'Armstrong'|'sarmstrong1@info...|  'Male'|    'Sports'| '3/31/2008'| 71869| 'Financial Advisor'|        2|\n",
      "|  3|     'Carr'|'fcarr2@woothemes...|  'Male'|'Automotive'| '7/12/2009'|101768|'Recruiting Manager'|        3|\n",
      "|  4|   'Murray'|   'jmurray3@gov.uk'|'Female'|  'Jewelery'|'12/25/2014'| 96897|'Desktop Support ...|        3|\n",
      "|  5|    'Ellis'|'jellis4@scienced...|'Female'|   'Grocery'| '9/19/2002'| 63702|'Software Enginee...|        7|\n",
      "+---+-----------+--------------------+--------+------------+------------+------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f92280b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "916f3b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = emp_df.sample(False, 0.1)\n",
    "sample_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3623787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_mngrs_df = emp_df.filter(\"salary >= 100000\")\n",
    "emp_mngrs_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d0e8095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|salary|\n",
      "+------+\n",
      "|101768|\n",
      "|118497|\n",
      "|108657|\n",
      "|108093|\n",
      "|121966|\n",
      "|141139|\n",
      "|106659|\n",
      "|148952|\n",
      "|109890|\n",
      "|115274|\n",
      "+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_mngrs_df.select(\"salary\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342d677f",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bceed63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f981090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('preprocessing').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6abf70",
   "metadata": {},
   "source": [
    "## Normalizing numeric data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106c919d",
   "metadata": {},
   "source": [
    "Normalizing is the process of mapping numeric data from its original range into a range from zero (0) to one (1). This is important, because we may have multiple attributes with different ranges. For example we have salaries, which might have ranges in the tens, and hundreds of thousands. Then, we might have another column, for example, Miles commuted to work, which might be on the order of tens of miles. The reason we want to normalize those attributes in a zero to one range is so that when algorithms that use distance as a measure, they don't weight some attributes, like salary, orders of magnitude, more heavily than others, like miles commuted to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1c03491",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = spark.createDataFrame([\n",
    "    (1, Vectors.dense([10.0, 10000.0, 1.0]),),\n",
    "    (2, Vectors.dense([20.0, 30000.0, 2.0]),),\n",
    "    (3, Vectors.dense([30.0, 40000.0, 3.0]),)],\n",
    "    [\"id\", \"features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230ae6fa",
   "metadata": {},
   "source": [
    "Each row of the dataframe will include an identifier and a list of numeric values. The first record will have an ID of one, and then it will have a set of features which we create as a dense vector, and that vector will include the number 10, the number 10,000, and the number 1. And we'll specify the columns for the dataframe; id and features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12222c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "877bf2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|id |features          |\n",
      "+---+------------------+\n",
      "|1  |[10.0,10000.0,1.0]|\n",
      "|2  |[20.0,30000.0,2.0]|\n",
      "|3  |[30.0,40000.0,3.0]|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e8bbaf",
   "metadata": {},
   "source": [
    "We are going to create a scaler object called \"feature_scaler\", to call the MinMaxScaler function, and to tell it that we want to transfer the input column, which is named \"features\", and we want that scaled version of that input column to go to a new output column, which is called \"sfeatures\", which is short for scaled features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c77de",
   "metadata": {},
   "source": [
    "This object will transform the contents of feature vectors into a scaled version, and save it into the \"sfeatures\" column. Then we'll fit the model to the data using the fit function. To do that, we'll create an object called \"smodel\", and that'll be set equal to the feature_scaler, and we'll apply the fit function, and the data we're going to fit is what's loaded into our features dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb335e08",
   "metadata": {},
   "source": [
    "The next thing we want to do is we want to call the transform function, and what this will do is it will apply the transformation and actually create the scaled dataset. So to do this, I'm going to create a new dataframe, called sfeatures, and this is going to be built using the smodel we just defined, and we're going to transform using the features dataframe. \n",
    "So what we've done is we've created a MinMaxScaler, we fit our data to it, and then we used the transform to create a new scaled_feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e9a16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"sfeatures\")\n",
    "smodel = feature_scaler.fit(features_df)\n",
    "sfeatures_df = smodel.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f487e3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]), sfeatures=SparseVector(3, {}))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfeatures_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fda633",
   "metadata": {},
   "source": [
    "What you'll notice here is, in addition to the ID and features that we had in our original dataframe, we now have a new column, called \"sfeature\"s, which has a dense vector, which is scaled, and it's in the zero to one range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4fd155f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|          features|           sfeatures|\n",
      "+------------------+--------------------+\n",
      "|[10.0,10000.0,1.0]|           (3,[],[])|\n",
      "|[20.0,30000.0,2.0]|[0.5,0.6666666666...|\n",
      "|[30.0,40000.0,3.0]|       [1.0,1.0,1.0]|\n",
      "+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sfeatures_df.select(\"features\", \"sfeatures\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83cc752",
   "metadata": {},
   "source": [
    "The scaled data is in the range from zero to one, and the larger the original value, the larger the scaled value. The smallest value in each column of the feature vector is mapped to zero, and the largest value is mapped to one. Values in between the minimum and maximum are scaled proportionally between zero and one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6727d7f",
   "metadata": {},
   "source": [
    "## Standardizing numeric data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d48693",
   "metadata": {},
   "source": [
    "In addition to normalizing, there's another operation that's often done to numeric data, and that's called standardizing. And basically the idea here is, we may have data that is pretty close to a bell-shaped curve, or normally distributed, but maybe not exactly. With standardization, what we can do is map the data into a range of -1 to +1 with a mean of 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6585802b",
   "metadata": {},
   "source": [
    "We do this because some machine learning algorithms, such as support vector machines, and some linear models work better when all of the features have a unit variance and a zero mean. So, what happens is when we apply standardization, our data is slightly shifted in its shape so that it becomes more normalized, or more like a bell curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4760c9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60178e03",
   "metadata": {},
   "source": [
    "Now let's create a scaler object. I want this to be standardized to a normal distribution, so I'm going to say withStd, for standardization, to true. And I want a mean around zero, so I'm going to say withMean equals true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df3479ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_stand_scaler = StandardScaler(inputCol=\"features\", outputCol=\"sfeatures\", withStd=True, withMean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740f9bcb",
   "metadata": {},
   "source": [
    "So what this is going to do is use the model we just built, and it's going to transform the data I'm passing in. And the data I want to transform is the features data frame. So now, I've created a model, I've fit data to the model, and then I've taken that same data and I've transformed it to this new standardized form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7379e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_smodel = feature_stand_scaler.fit(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fbdf5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_sfeatures_df = stand_smodel.transform(features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43253c64",
   "metadata": {},
   "source": [
    "We've created a dataframe that has the features with the original data, as well as another column which has the features mapped to a normal distribution, with a mean of zero and a minimum range of about negative one and a maximum of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c9471a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]), sfeatures=DenseVector([-1.0, -1.0911, -1.0]))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stand_sfeatures_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abc77104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------------------------------+\n",
      "|id |features          |sfeatures                     |\n",
      "+---+------------------+------------------------------+\n",
      "|1  |[10.0,10000.0,1.0]|[-1.0,-1.091089451179962,-1.0]|\n",
      "|2  |[20.0,30000.0,2.0]|[0.0,0.2182178902359923,0.0]  |\n",
      "|3  |[30.0,40000.0,3.0]|[1.0,0.8728715609439696,1.0]  |\n",
      "+---+------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stand_sfeatures_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2a5703",
   "metadata": {},
   "source": [
    "## Bucketizing numeric data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870f4ca7",
   "metadata": {},
   "source": [
    "Now let's take a look at how we can organize continuous ranges of data into buckets or partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c27ba543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93321da3",
   "metadata": {},
   "source": [
    "Bucketizer allows us to group data based on boundaries, and so I need to provide a list of boundaries for Bucketizer to work with. So I call those boundaries \"splits\". And I'm going to provide a list of what these splits are. Now at the lower end, I would like anything starting at negative infinity to go in the first bucket. From negative infinity up to -10 will be one bucket and then from -10 to 0 will be another bucket from 0 to 10 will be my next bucket and everything that's greater than 10 and up to positive infinity, will go in the last bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f44684e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [-float(\"inf\"), -10.0, 0.0, 10.0, float(\"inf\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a161aa",
   "metadata": {},
   "source": [
    "What we're going to do here is create a list, but this is going to be a list of rows, the syntax my look a little odd because I'm using parentheses, that's because I want each of these to be mapped to a distinct row in the data frame later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5f9597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_data = [(-800.0,), (-10.5,), (-1.7,), (0.0,), (8.2,), (90.1,)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06e26d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_df = spark.createDataFrame(b_data, [\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71d793c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|features|\n",
      "+--------+\n",
      "|  -800.0|\n",
      "|   -10.5|\n",
      "|    -1.7|\n",
      "|     0.0|\n",
      "|     8.2|\n",
      "|    90.1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39083e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketizer = Bucketizer(splits=splits, inputCol=\"features\", outputCol=\"bfeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dbe48793",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketed_df = bucketizer.transform(b_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af6210ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|features|bfeatures|\n",
      "+--------+---------+\n",
      "|  -800.0|      0.0|\n",
      "|   -10.5|      0.0|\n",
      "|    -1.7|      1.0|\n",
      "|     0.0|      2.0|\n",
      "|     8.2|      2.0|\n",
      "|    90.1|      3.0|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b9d46",
   "metadata": {},
   "source": [
    "Why didn't we have to do a fit? That's because Bucketizing is fairly simple and my splits is the list of the boundaries I want for each bucket, so there's no need to fit data, so we can skip the normal fitting operation and go right to the transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d67d81",
   "metadata": {},
   "source": [
    "## Tokenizing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d3706",
   "metadata": {},
   "source": [
    "Now let's shift our focus to working with text data. And in this dataframe we'll have three rows - sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef032c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c3c9095",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = spark.createDataFrame([\n",
    "    (1, \"This is an introduction to Spark MLlib\"),\n",
    "    (2, \"MLlib includes libraries for classification and regression\"),\n",
    "    (3, \"It also contains supporting tools for pipelines\")],\n",
    "    [\"id\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e81befd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------------+\n",
      "|id |sentence                                                  |\n",
      "+---+----------------------------------------------------------+\n",
      "|1  |This is an introduction to Spark MLlib                    |\n",
      "|2  |MLlib includes libraries for classification and regression|\n",
      "|3  |It also contains supporting tools for pipelines           |\n",
      "+---+----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "caf93e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b2039dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenized_df = sent_token.transform(sentences_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4fffe0",
   "metadata": {},
   "source": [
    "We now have a third column \"words\", in addition to the ID and the sentences. And the words column contains lists of words that have been broken up in the ways you would normally expect a regular expression pattern matching to break up a sentence into words. So based on white space, punctuation, etc. So this is how we can use Tokenizer to split up strings into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37d670d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------------+------------------------------------------------------------------+\n",
      "|id |sentence                                                  |words                                                             |\n",
      "+---+----------------------------------------------------------+------------------------------------------------------------------+\n",
      "|1  |This is an introduction to Spark MLlib                    |[this, is, an, introduction, to, spark, mllib]                    |\n",
      "|2  |MLlib includes libraries for classification and regression|[mllib, includes, libraries, for, classification, and, regression]|\n",
      "|3  |It also contains supporting tools for pipelines           |[it, also, contains, supporting, tools, for, pipelines]           |\n",
      "+---+----------------------------------------------------------+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_tokenized_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d4dcc1",
   "metadata": {},
   "source": [
    "We don't have to call the fit function, because we're not actually fitting data. The Tokenizer already knows how to do its job, which is basically how to split up strings into separate words. We can go right from creating a Tokenizer object, to the transformation process. There's no fit operation needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665ba89b",
   "metadata": {},
   "source": [
    "## TF-IDF (term frequency-inverse document frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4161a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a81c35e",
   "metadata": {},
   "source": [
    "So again we start with some text. For example, here's a sentence. And we tokenize it so we get a list of words. And then we count the number of times a particular word appears. So for example normalizing appears only once, so it has a count of one. The word to shows up twice, so it has a count of two. And we go through and we do this for all of the documents in our corpus. And corpus is just another name for a collection of documents. And we count up how often a term appears across all of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ed5ae0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentence='This is an introduction to Spark MLlib')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6487c303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentence='This is an introduction to Spark MLlib', words=['this', 'is', 'an', 'introduction', 'to', 'spark', 'mllib'])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenized_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "115dd921",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6d418e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_hfTF_df = hashingTF.transform(sent_tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a85a31fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentence='This is an introduction to Spark MLlib', words=['this', 'is', 'an', 'introduction', 'to', 'spark', 'mllib'], rawFeatures=SparseVector(20, {6: 2.0, 8: 1.0, 9: 1.0, 10: 1.0, 13: 1.0, 15: 1.0}))]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_hfTF_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c117d533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "| id|            sentence|               words|         rawFeatures|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  1|This is an introd...|[this, is, an, in...|(20,[6,8,9,10,13,...|\n",
      "|  2|MLlib includes li...|[mllib, includes,...|(20,[2,4,11,12,15...|\n",
      "|  3|It also contains ...|[it, also, contai...|(20,[1,4,6,8,11,1...|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_hfTF_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5723d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"idf_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a948188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "idfModel = idf.fit(sent_hfTF_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec78cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = idfModel.transform(sent_hfTF_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "51946457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, sentence='This is an introduction to Spark MLlib', words=['this', 'is', 'an', 'introduction', 'to', 'spark', 'mllib'], rawFeatures=SparseVector(20, {6: 2.0, 8: 1.0, 9: 1.0, 10: 1.0, 13: 1.0, 15: 1.0}), idf_features=SparseVector(20, {6: 0.5754, 8: 0.2877, 9: 0.6931, 10: 0.6931, 13: 0.6931, 15: 0.2877}))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dcaa60ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|            sentence|               words|         rawFeatures|        idf_features|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|  1|This is an introd...|[this, is, an, in...|(20,[6,8,9,10,13,...|(20,[6,8,9,10,13,...|\n",
      "|  2|MLlib includes li...|[mllib, includes,...|(20,[2,4,11,12,15...|(20,[2,4,11,12,15...|\n",
      "|  3|It also contains ...|[it, also, contai...|(20,[1,4,6,8,11,1...|(20,[1,4,6,8,11,1...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

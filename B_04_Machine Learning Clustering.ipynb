{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8de8cfff",
   "metadata": {},
   "source": [
    "-- Notepad to myself --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9770cad4",
   "metadata": {},
   "source": [
    "# Machine Learning with DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69d2b8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581e4a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "063f96a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_path = data_path + \"utilization.json\"\n",
    "df2 = spark.read.json(df2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03455e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2_csv_path = data_path + \"utilization.csv\"\n",
    "#df2 = spark.read.csv(df2_csv_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c396017b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|event_datetime     |free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|0.57           |03/05/2019 08:06:14|0.51       |100      |47           |\n",
      "|0.47           |03/05/2019 08:11:14|0.62       |100      |43           |\n",
      "|0.56           |03/05/2019 08:16:14|0.57       |100      |62           |\n",
      "|0.57           |03/05/2019 08:21:14|0.56       |100      |50           |\n",
      "|0.35           |03/05/2019 08:26:14|0.46       |100      |43           |\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59472ba",
   "metadata": {},
   "source": [
    "### ML - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ad158",
   "metadata": {},
   "source": [
    "A commonly used technique in exploratory data analysis is called **clustering**. And here the idea is that we want to see if there are natural groupings among the data. So for example, let's take a look at the utilization data. Let's see if we can divide that dataset into three groups that logically come together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54317374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10982196",
   "metadata": {},
   "source": [
    "A vector is basically like an array or a single data structure that holds all the values from a particular row that the machine learning algorithm will be looking at. The machine learning algorithms in the PySpark-ML packages expect the input data to be in a *single vector* like in scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6845d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=[\"cpu_utilization\", \"free_memory\", \"session_count\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "894e8f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vcluster = vectorAssembler.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44173705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+----------------+\n",
      "|cpu_utilization|event_datetime     |free_memory|server_id|session_count|features        |\n",
      "+---------------+-------------------+-----------+---------+-------------+----------------+\n",
      "|0.57           |03/05/2019 08:06:14|0.51       |100      |47           |[0.57,0.51,47.0]|\n",
      "|0.47           |03/05/2019 08:11:14|0.62       |100      |43           |[0.47,0.62,43.0]|\n",
      "|0.56           |03/05/2019 08:16:14|0.57       |100      |62           |[0.56,0.57,62.0]|\n",
      "|0.57           |03/05/2019 08:21:14|0.56       |100      |50           |[0.57,0.56,50.0]|\n",
      "|0.35           |03/05/2019 08:26:14|0.46       |100      |43           |[0.35,0.46,43.0]|\n",
      "|0.41           |03/05/2019 08:31:14|0.58       |100      |48           |[0.41,0.58,48.0]|\n",
      "|0.57           |03/05/2019 08:36:14|0.35       |100      |58           |[0.57,0.35,58.0]|\n",
      "|0.41           |03/05/2019 08:41:14|0.4        |100      |58           |[0.41,0.4,58.0] |\n",
      "|0.53           |03/05/2019 08:46:14|0.35       |100      |62           |[0.53,0.35,62.0]|\n",
      "|0.51           |03/05/2019 08:51:14|0.6        |100      |45           |[0.51,0.6,45.0] |\n",
      "+---------------+-------------------+-----------+---------+-------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vcluster.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a0e18c",
   "metadata": {},
   "source": [
    "#### K-means\n",
    "\n",
    "So, what we're taking the features from the vcluster dataframe, fit it to the kmeans model that we just specified, and keep the results in a machine learning model called kmodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4aa9345",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans().setK(3) #number of clusters\n",
    "kmeans = kmeans.setSeed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f02428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmodel = kmeans.fit(df_vcluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31486d7",
   "metadata": {},
   "source": [
    "The critical thing in a kmeans model is the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03b2877e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.71174897,  0.28808911, 86.87510507]),\n",
       " array([ 0.61918113,  0.38080285, 68.75004716]),\n",
       " array([ 0.51439668,  0.48445202, 50.49452021])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmodel.clusterCenters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a6c7f1",
   "metadata": {},
   "source": [
    "What we find is we have a set of three centers, and each center is specified by three values (our dimensions are CPU utilization, free memory and session count). And each of these values indicate the center of one of the three clusters. And all of the rows in our utilization dataframe fit or fall into one of these three clusters, and we can determine that by measuring the distance from the feature vector of each row to each of these centers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78d6af48",
   "metadata": {},
   "source": [
    "-- Notepad to myself --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426d0918",
   "metadata": {},
   "source": [
    "# Apache Spark Ecosystem\n",
    "\n",
    "Over the last couple of years Apache Spark has evolved into the Big Data platform of choice. There are a few really good reasons why it's become so popular. If we head over to the [Apache Spark website](http://spark.apache.org), we can see some of the reasons that people are using it. It's fast, it's easy to develop in because it has a couple of language APIs, meaning that you can access it using *Scala, Java, Python, R, or SQL*. And it has its own ecosystem. Good news for Python-ists, then *PySpark* allows us to access the power of Apache Spark.\n",
    "\n",
    "The developers of Apache Spark wanted it to be fast to work on their large datasets. But when they were working on their big data projects, many of the scripting languages did not fit the bill. Because Spark runs on a Java Virtual Machine *(JVM)*, it lets us call into other Java-based Big Data systems such as *Cassandra, HTFS*, and *Hbase*. Spark runs locally as well as in a cluster, on premise, or in a cloud. It runs on top of *Hadoop YARN, Apache Mesos, standalone*, or in the *Cloud*, such as *Amazon EC2* or *IBM Bluemix*. Although Spark is designed to work on large, distributed clusters, we can also use it on a single machine mode. The means of accessing these different modules is via the Spark Core API, which is the foundation of the Apache Spark Ecosystem.\n",
    "\n",
    "The Apache Spark Ecosystem consist of Spark SQL and Dataframes, Streaming (for real time data Mllib for machine learning), and GraphX (graph-structured data at scale).\n",
    "\n",
    "**Spark Core** contains the basic functionality of Spark including components for task scheduling, memory management, fault recovery, interacting with storage systems and more. Remember that Spark is primarily written in Scala so this is the default language. However, we can also use Java, Python, R, and SQL, so imagine how accessible this makes Spark to a wide variety of developers. \n",
    "\n",
    "**Spark SQL** allows us to interactively explore the data using SQL queries. This is done by using a programming abstraction called *DataFrames*. Spark SQL can act as a distributed SQL query engine. Now if you're familiar with dataframes, either from Pandas or R, you know how easy they are to manage. This is where I think the Spark developers were really smart because by creating this DataFrame abstraction it meant that data analysts and data scientists who are familiar with these from Pandas and R could get up and running with Apache Spark really quickly. Spark SQL allows us to intermix SQL's queries with the programmatic data manipulations supported by lower level APIs in Python, Java, and Scala all within a single application and so combining SQL with complex analytics.\n",
    "\n",
    "**Spark Streaming**; Many applications need the ability to process and analyze not only batch data, but also streams of new data in real time. Running on top of Spark, Spark Streaming enables powerful interactive and analytical applications across both streaming and historical data. The real bonus is the fact that we can use virtually the same code that we created for batch data to process real time data. \n",
    "\n",
    "**MLlib** built on top of Spark is a scalable machine learning library that delivers both high quality algorithms, for example, multiple iterations to increase accuracy, and because the work is completed in memory, it's fast. And it can be 100 times faster than *MapReduce* (another big data abstraction). \n",
    "\n",
    "**GraphX** is a graph computation engine built on top of Spark that enables users to interactively build, transform, and reason about graph structure data at scale. It comes complete with a library of common algorithms. GraphX introduces a new graph abstraction, which is a directed multigraph with properties attached to each vertex and edge. GraphX is really helpful for social networks where we can model as a graph and then we can determine relationships between different nodes or vertices. \n",
    "\n",
    "As a result, Spark has a pretty impressive and self-sufficient ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c8645e",
   "metadata": {},
   "source": [
    "## What/Why is PySpark?\n",
    "\n",
    "Spark is originally written in Scala. So PySpark is just a Python wrapper around the Spark Core. But why would I use Spark instead of *pandas, Hadoop* or *Dask*? And more importantly, when would I use one of these?\n",
    "\n",
    "**Pandas** is great for tabular data. As far as data wrangling is concerned, there are several more options and features available compared to Spark because it's been around longer. Pandas can handle hundreds of thousands if not millions of rows. But what happens when our data is so large that it has to be stored across several computers or our computer just doesn't have the processing capability to process the data quickly enough. Now unfortunately, at this point, we have to move away from pandas as it isn't a distributed system and find another solution. And that's exactly what Apache Spark does and PySpark makes it easy for us to use Apache Spark in Python. \n",
    "\n",
    "**Hadoop** is a distributed cluster. Until a couple of years ago, Hadoop was the one and only Big Data platform. Hadoop has a compute system called *MapReduce*, and a storage system called the *Hadoop File System*. This allowed us to get the benefits of clustering several commodity service together and was designed for local storage. The only problem is that they're closely integrated and so it's really difficult to run one without the other. Public cloud is one example of this. We can get AWS or Azure or Google Cloud Storage separately from compute. Spark has the advantage that we can use it on Hadoop storage, or in a public Cloud environment. The thing is, if we have a single machine, it's unlikely to crash but if we're working with several machines, then one of them will probably crash at some point. So how can we make sure we don't lose any of the data if one of the machines crashes? Distributed systems like Hadoop have a *Hadoop Distributed File System (HDFS)* that splits the files into chunks called blocks and then replicates the blocks across several machines. If one of the machines fail, HDFS will just request that block from another machine that has it. Now one of the key differences between Spark and Hadoop lies on their approach to processing. Spark can do it in memory, while Hadoop MapReduce has to read from and write to a disk. As a result, the speed of processing differ significantly and Spark can be up to 100 times faster. The general rule of thumb for an on-prem installation is that Hadoop requires more memory on disk and Spark requires more RAM (meaning that setting up Spark clusters can be more expensive). We would choose Hadoop mainly for disc-heavy operations with the MapReduce paradigm and Spark tends to be more flexible, but more costly in-memory processing architecture. \n",
    "\n",
    "**Dask** is a library for parallel computing in Python. It's less obvious when we should be using PySpark versus Dask. Spark is written in Scala, but has support for Java, Python, R and SQL and interpolates well with JVM code. Dask on the other hand, is only written in Python and only really supports Python. Spark is an all in one project so it has its own ecosystem. In the case of Dask, It's part of a larger Python ecosystem and works really well with other Python libraries such as numpy, pandas and scikit-learn. Sparks Dataframe has its own API and implements a good chunk of the SQL language. It also has a high level query optimizer for complex queries. Dask on the other hand, uses the Pandas API (great at things like time series operations, indexing) but Dask doesn't support SQL. Spark support for streaming is brilliant and we can get great performance on large streaming operations. Dask requires a lot more work to be able to work on streaming. Spark MLlib has great support for common machine learning operations. Dask on the other hand, relies on and intraoperates with Python's well known scikit-learn library and so we might get a little better performance here. Finally, Sparks GraphX library allows us to do graph processing. Dask on the other hand doesn't have a specific library to do graph processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeb9aca",
   "metadata": {},
   "source": [
    "## Spark origins\n",
    "\n",
    "See -> https://spark.apache.org/history.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bda16bd",
   "metadata": {},
   "source": [
    "## Spark Components\n",
    "\n",
    "The Driver sits on a node on the Cluster and does a couple of things; it maintains information about the Spark application. So it'll do things like respond to a users program or input and it distributes and schedules work across the Executors. The Driver process is critical and maintains all relevant information for the Spark application. The Executors on the Worker Node carries out the work that's been assigned by the Driver and it reports back on the state of the computation, back to the Driver. It's important to remember that the Driver and the Executor are just processes. This means that they can all exist on one machine if we're running on local nodes so they just be threads and they can run on different machines if we're running on a cluster. Finally, we can think of the Cluster Manager as task managers or resource managers. So, when we submit Spark applications to Cluster Managers, they'll grant resources to our applications so that the work could be completed. \n",
    "\n",
    "Let's take a quick look at the different types of Cluster Managers such as Standalone, Apache Mesos or Hadoop YARN (a general Cluster Manager) and Kubernetes (to automate to the deployment and management of containerized applications). \n",
    "\n",
    "Now, one of the first things we'll want to do for any Spark application is to create a *SparkSession*. Back then, we've worked with previous versions of Spark, (SparkContext, SQL-Context, and Hive-Context). The SparkContext allows us to communicate with some of Spark's lower level APIs, such as *RDDs*. The SQL-Context gives us access to higher level APIs, such as the Spark-SQL. \n",
    "\n",
    "With Spark version 2.0, the functionality previously available only through the SparkContext and the SQL-Context is now available via the **SparkSession**. This means that the SparkSession is now our single, unified entry point to manipulation data with Spark. That's great because it reduces the number of concepts we need to remember. \n",
    "\n",
    "We'll be using PySpark which is a wrapper around the Spark Core. So, when we start our SparkSession in Python, what's actually happening in the background is that PySpark uses *PY4G* to launch a Java Virtual Machine (JVM) and creates a Java SparkContext. All PY4G does is allow Python programs to dynamically access Java objects in a JVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c619a33",
   "metadata": {},
   "source": [
    "## Partitions\n",
    "\n",
    "Earlier, we summarized about how Spark is a distributed system. This means that if we want the workers to work in parallel, Spark needs to break the data into chunks or partitions. A partition is a collection of rows from our Dataframes that sits on one machine in our cluster. So a Dataframes partition is how the data is physically distributed across the cluster of machines during execution. Now because we're working with a high level API when using Dataframes, we don't normally get involved with manipulating the partitions manually. If we only have one partition, Spark cannot parallelize jobs even if we have a cluster of machines available. In the same way, if we have several partitions but only one worker, Spark cannot parallelize jobs as there's only one resource that can do the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff1f36d",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "Transformations are a core data structure in Spark, and are immutable. Immutable is just a fancy way of saying they cannot be changed once they've been created. So the instructions that we use to modify the Dataframe are known as *Transformations*. This could be a filter operation, or a distinct operation, where we only get the unique values in a certain column. When we perform Transformation operations, nothing seems to happen. Spark doesn't act on Transformations until we perform something called *Actions*. \n",
    "\n",
    "When an operation is performed on our data, Spark doesn't work on modifying the data straight away. Instead, it builds a plan of Transformations that will be performed on the data. This waiting until the last moment to do the work is known as a *Lazy Evaluation*. It's really helpful, because it allows Spark to create a streamlined plan, allowing it to run as efficiently as possible across the cluster. This makes really good sense for Big Data jobs. \n",
    "\n",
    "Let's say we have a log file with millions of rules of data. We then decide to filter all of the ones that were identified by a certain type of error. And then finally, we call a function to fetch only one of those filtered rules. What would Spark's Lazy Evaluation do? Well, it just needs to find the first occurrence of the type of error message in any of the partitions, because all we wanted was one rule. \n",
    "\n",
    "Let's compare that with what would have happened if we didn't use the Lazy Evaluations. Well we would start off with the log file, we then filter by errors in all of the partitions, then store these intermediate results somewhere, even though all we were interested in was a single rule with a certain type of error. So Lazy Evaluation is pretty efficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d021a6",
   "metadata": {},
   "source": [
    "## Actions\n",
    "\n",
    "An *Action* tells Spark to compute the results of these Transformations, and there are three types;\n",
    "\n",
    "- View data e.g. show()\n",
    "- Collect data e.g. collect() - collects the data to our driver\n",
    "- Write data e.g. write.format(..) - allows us to write to output data sources. \n",
    "\n",
    "If we take our log file illustration, if we filtered by a certain kind of error, this would be a Transformation. But if we then formed an aggregation, such as a count of that type of error, this counting would take place over all of the partitions, and this counting operation would be an *Action*. If we then run a collect(), this would be another Action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321bf7d8",
   "metadata": {},
   "source": [
    "## DataFrame API\n",
    "\n",
    "There are two main APIs; *DataFrames* and *RDDs* (Resilient Distributed Datasets). The *DataFrames* are the high level APIs and the *RDDs* are the low level APIs. DataFrames are easy to get started with and cover a good chunk of what we'll need to know on the job. \n",
    "\n",
    "When Spark was first open source, Spark enabled distributed data processing using RDDs. This provided a simple API for distributed data processing. So Big Data Engineers who are familiar with MapReduce jobs could now leverage the power of distributed processing using general purpose programming languages, such as Java, Python, R, and Scala.\n",
    "\n",
    "Now the challenge was that if Apache Spark wanted to attract a wider audience onto their platform, including data analysts and data scientists, then they were going to have to create something that they would be familiar with. If there's one thing that data scientists with R or a Pandas background are familiar with it's a *Dataframe*. In Spark a *DataFrame* is a distributed collection of objects of type rule. We can think of this as a table in a Relational Database or an Excel document, except there are some significant optimizations taking place under the hood. So while a table in Excel will sit on a single computer, a Spark DataFrame could sit across hundreds of computers. We can also create a DataFrame from a wide variety of sources, such as structured data files, tables in Hive, external databases, or even existing RDDs. \n",
    "\n",
    "As a keynote about the *Dataset API*, Datasets are an API that we use when using a statically typed language, like Java or Scala. Because Python is a dynamically typed language, it doesn't support the Dataset API, but fortunately many of the benefits of the Dataset API are already available via DataFrames."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Notepad to myself --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_path = data_path + \"utilization.json\"\n",
    "df2 = spark.read.json(df2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2_csv_path = data_path + \"utilization.csv\"\n",
    "#df2 = spark.read.csv(df2_csv_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|event_datetime     |free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|0.57           |03/05/2019 08:06:14|0.51       |100      |47           |\n",
      "|0.47           |03/05/2019 08:11:14|0.62       |100      |43           |\n",
      "|0.56           |03/05/2019 08:16:14|0.57       |100      |62           |\n",
      "|0.57           |03/05/2019 08:21:14|0.56       |100      |50           |\n",
      "|0.35           |03/05/2019 08:26:14|0.46       |100      |43           |\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView(\"utilization\") #for use of Spark-SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+------------------+------------------+------------------+\n",
      "|summary|    cpu_utilization|     event_datetime|       free_memory|         server_id|     session_count|\n",
      "+-------+-------------------+-------------------+------------------+------------------+------------------+\n",
      "|  count|             500000|             500000|            500000|            500000|            500000|\n",
      "|   mean| 0.6205177400000123|               null|0.3791280999999977|             124.5|          69.59616|\n",
      "| stddev|0.15875173872912837|               null|0.1583093127837622|14.430884120553253|14.850676696352865|\n",
      "|    min|               0.22|03/05/2019 08:06:14|               0.0|               100|                32|\n",
      "|    max|                1.0|04/09/2019 01:22:46|              0.78|               149|               105|\n",
      "+-------+-------------------+-------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.47047715730807443"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.stat.corr('cpu_utilization', 'free_memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5008320848876572"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.stat.corr('session_count', 'free_memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+\n",
      "| server_id_freqItems|session_count_freqItems|\n",
      "+--------------------+-----------------------+\n",
      "|[146, 137, 101, 1...|   [92, 101, 83, 104...|\n",
      "+--------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.stat.freqItems(('server_id', 'session_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+----------+\n",
      "|min_cpu|mean_cpu|max_cpu|stddev_cpu|\n",
      "+-------+--------+-------+----------+\n",
      "|   0.22|    0.62|    1.0|      0.16|\n",
      "+-------+--------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT min(cpu_utilization) min_cpu, ROUND(mean(cpu_utilization),2) mean_cpu, \\\n",
    "                  max(cpu_utilization) max_cpu, ROUND(stddev(cpu_utilization),2) stddev_cpu \\\n",
    "          FROM utilization') \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------+-------+----------+\n",
      "|server_id|min_cpu|mean_cpu|max_cpu|stddev_cpu|\n",
      "+---------+-------+--------+-------+----------+\n",
      "|      100|   0.27|    0.47|   0.67|      0.12|\n",
      "|      101|    0.6|     0.8|    1.0|      0.12|\n",
      "|      102|   0.56|    0.76|   0.96|      0.12|\n",
      "|      103|   0.56|    0.76|   0.96|      0.12|\n",
      "|      104|   0.51|    0.71|   0.91|      0.12|\n",
      "|      105|   0.29|    0.49|   0.69|      0.12|\n",
      "|      106|   0.22|    0.42|   0.62|      0.12|\n",
      "|      107|   0.45|    0.65|   0.85|      0.12|\n",
      "|      108|   0.55|    0.75|   0.95|      0.12|\n",
      "|      109|   0.36|    0.56|   0.76|      0.12|\n",
      "+---------+-------+--------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT server_id, min(cpu_utilization) min_cpu, ROUND(mean(cpu_utilization),2) mean_cpu, \\\n",
    "                  max(cpu_utilization) max_cpu, ROUND(stddev(cpu_utilization),2) stddev_cpu \\\n",
    "           FROM utilization \\\n",
    "           GROUP BY server_id \\\n",
    "           ORDER BY server_id ASC') \\\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bucketizing\n",
    "\n",
    "To calculate statistics on buckets or histograms: the idea here is, rather than look at each server individually, let's bucket values according to how frequently they occur in certain ranges. So if we want to know how often does a CPU utilization fall in the range of 1-10 or 11-20 or 21-30, all the way up to 91-100, we could put each of those CPU utilization measures into its own bucket and count how many times a CPU utilization goes into that bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+------+\n",
      "|server_id|cpu_utilization|bucket|\n",
      "+---------+---------------+------+\n",
      "|      100|           0.57|     5|\n",
      "|      100|           0.47|     4|\n",
      "|      100|           0.56|     5|\n",
      "|      100|           0.57|     5|\n",
      "|      100|           0.35|     3|\n",
      "|      100|           0.41|     4|\n",
      "|      100|           0.57|     5|\n",
      "|      100|           0.41|     4|\n",
      "|      100|           0.53|     5|\n",
      "|      100|           0.51|     5|\n",
      "+---------+---------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT server_id, cpu_utilization, FLOOR(cpu_utilization*100/10) bucket \\\n",
    "          FROM utilization') \\\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|bucket| count|\n",
      "+------+------+\n",
      "|    10|    57|\n",
      "|     9| 20207|\n",
      "|     8| 56598|\n",
      "|     7| 88242|\n",
      "|     6|116725|\n",
      "|     5|104910|\n",
      "|     4| 68046|\n",
      "|     3| 37029|\n",
      "|     2|  8186|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT FLOOR(cpu_utilization*100/10) bucket, \\\n",
    "                  count(*) count \\\n",
    "          FROM utilization \\\n",
    "          GROUP BY bucket \\\n",
    "          ORDER BY bucket DESC') \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-series Analysis (window function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+------------------+\n",
      "|event_datetime     |server_id|cpu_utilization|avg_server_util   |\n",
      "+-------------------+---------+---------------+------------------+\n",
      "|03/05/2019 08:06:31|110      |0.68           |0.5537749999999892|\n",
      "|03/05/2019 08:11:31|110      |0.58           |0.5537749999999892|\n",
      "|03/05/2019 08:16:31|110      |0.55           |0.5537749999999892|\n",
      "|03/05/2019 08:21:31|110      |0.63           |0.5537749999999892|\n",
      "|03/05/2019 08:26:31|110      |0.63           |0.5537749999999892|\n",
      "|03/05/2019 08:31:31|110      |0.71           |0.5537749999999892|\n",
      "|03/05/2019 08:36:31|110      |0.67           |0.5537749999999892|\n",
      "|03/05/2019 08:41:31|110      |0.55           |0.5537749999999892|\n",
      "|03/05/2019 08:46:31|110      |0.37           |0.5537749999999892|\n",
      "|03/05/2019 08:51:31|110      |0.7            |0.5537749999999892|\n",
      "+-------------------+---------+---------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT event_datetime, server_id, cpu_utilization, \\\n",
    "                  avg(cpu_utilization) OVER (PARTITION BY server_id) avg_server_util \\\n",
    "          FROM utilization\") \\\n",
    ".show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+---------------+-----------------+\n",
      "|event_datetime     |server_id|cpu_utilization|avg_server_util|delta_server_util|\n",
      "+-------------------+---------+---------------+---------------+-----------------+\n",
      "|03/05/2019 08:06:31|110      |0.68           |0.55           |0.13             |\n",
      "|03/05/2019 08:11:31|110      |0.58           |0.55           |0.03             |\n",
      "|03/05/2019 08:16:31|110      |0.55           |0.55           |0.0              |\n",
      "|03/05/2019 08:21:31|110      |0.63           |0.55           |0.08             |\n",
      "|03/05/2019 08:26:31|110      |0.63           |0.55           |0.08             |\n",
      "|03/05/2019 08:31:31|110      |0.71           |0.55           |0.16             |\n",
      "|03/05/2019 08:36:31|110      |0.67           |0.55           |0.12             |\n",
      "|03/05/2019 08:41:31|110      |0.55           |0.55           |0.0              |\n",
      "|03/05/2019 08:46:31|110      |0.37           |0.55           |-0.18            |\n",
      "|03/05/2019 08:51:31|110      |0.7            |0.55           |0.15             |\n",
      "+-------------------+---------+---------------+---------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT event_datetime, server_id, cpu_utilization, \\\n",
    "                  ROUND(avg(cpu_utilization) OVER (PARTITION BY server_id), 2) avg_server_util, \\\n",
    "                  ROUND(cpu_utilization - avg(cpu_utilization) OVER (PARTITION BY server_id), 2) delta_server_util \\\n",
    "          FROM utilization\") \\\n",
    ".show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+------------------+\n",
      "|event_datetime     |server_id|cpu_utilization|avg_server_util   |\n",
      "+-------------------+---------+---------------+------------------+\n",
      "|03/05/2019 08:06:31|110      |0.68           |0.63              |\n",
      "|03/05/2019 08:11:31|110      |0.58           |0.6033333333333334|\n",
      "|03/05/2019 08:16:31|110      |0.55           |0.5866666666666666|\n",
      "|03/05/2019 08:21:31|110      |0.63           |0.6033333333333334|\n",
      "|03/05/2019 08:26:31|110      |0.63           |0.6566666666666666|\n",
      "|03/05/2019 08:31:31|110      |0.71           |0.6699999999999999|\n",
      "|03/05/2019 08:36:31|110      |0.67           |0.6433333333333333|\n",
      "|03/05/2019 08:41:31|110      |0.55           |0.5300000000000001|\n",
      "|03/05/2019 08:46:31|110      |0.37           |0.54              |\n",
      "|03/05/2019 08:51:31|110      |0.7            |0.58              |\n",
      "+-------------------+---------+---------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT event_datetime, server_id, cpu_utilization, \\\n",
    "                  avg(cpu_utilization) OVER (PARTITION BY server_id ORDER BY event_datetime \\\n",
    "                                             ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) avg_server_util \\\n",
    "          FROM utilization\") \\\n",
    ".show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.68+0.58)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6033333333333334"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.68+0.58+0.55)/3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
